# -*- coding: utf-8 -*-
"""FineTune_T5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dGPwFvUrlTYu79Js24MwXhU8ZU9bkS_O
"""


import argparse
import glob
import os
import json
import time
import logging
import random
import re
from itertools import chain
from string import punctuation

import pandas as pd
import numpy as np
import torch
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
import pytorch_lightning as pl
from sklearn.model_selection import train_test_split
import textwrap

from transformers import (
    AdamW,
    T5ForConditionalGeneration,
    T5Tokenizer,
    get_linear_schedule_with_warmup
)

MODEL_NAME = "t5-base"

tokenizer= T5Tokenizer.from_pretrained(MODEL_NAME)



def exctract_qas_CliCr(file_path: Path):
  with file_path.open() as json_file:
    data = json.load(json_file)
  docs = data["data"]
  data_rows = []
  for doc in docs:
    doc = doc["document"]
    context = doc["context"]
    for qas in doc["qas"]:
      question = qas["query"]
      for answer in qas["answers"]:
        answer_text = answer["text"]
        data_rows.append({"question":question,
                          "context":context,
                          "answer_text":answer_text})
  return pd.DataFrame(data_rows)




class ClicrQADataset(Dataset):
  def __init__(
      self,
      data:pd.DataFrame,
      tokenizer:T5Tokenizer,
      source_max_token_len: int = 396,
      target_max_token_len: int = 32,
      ):
    self.data =  data
    self.tokenizer =  tokenizer
    self.source_max_token_len =  source_max_token_len
    self.target_max_token_len =  target_max_token_len
  def __len__(self):
    return len(self.data)
  def __getitem__(self, index: int):
    data_row = self.data.iloc[index]
    source_encoding = tokenizer(
      data_row['question'],
      data_row['context'],
      max_length=self.source_max_token_len,
      padding='max_length',
      truncation="only_second",
      return_attention_mask=True,
      add_special_tokens=True,
      return_tensors="pt"
      )
    target_encoding = tokenizer(
      data_row['answer_text'],
      max_length=self.target_max_token_len,
      padding='max_length',
      truncation=True,
      return_attention_mask=True,
      add_special_tokens=True,
      return_tensors="pt"
      )
    labels = target_encoding['input_ids']
    labels[labels==0] = -100
    return dict(
        question=data_row['question'],
        context=data_row['context'],
        answer_text=data_row['answer_text'],
        input_ids=source_encoding["input_ids"].flatten(),
        attention_mask=source_encoding['attention_mask'].flatten(),
        labels=labels.flatten()
    )


class ClicrDataModule(pl.LightningDataModule):
  def __init__(
      self,
      train_df: pd.DataFrame,
      test_df: pd.DataFrame,
      val_df: pd.DataFrame,
      tokenizer:T5Tokenizer,
      batch_size: int = 8,
      source_max_token_len: int = 396,
      target_max_token_len: int = 32,
      ):
    super().__init__()
    self.train_df = train_df
    self.test_df = test_df
    self.val_df = val_df
    self.tokenizer = tokenizer
    self.batch_size = batch_size
    self.source_max_token_len = source_max_token_len
    self.target_max_token_len = target_max_token_len
  def setup(self):
    self.train_dataset = ClicrQADataset(
        self.train_df,
        self.tokenizer,
        self.source_max_token_len,
        self.target_max_token_len
        )
    self.test_dataset = ClicrQADataset(
    self.test_df,
    self.tokenizer,
    self.source_max_token_len,
    self.target_max_token_len
    )
    self.val_dataset = ClicrQADataset(
    self.val_df,
    self.tokenizer,
    self.source_max_token_len,
    self.target_max_token_len
    )
  def train_dataloader(self):
    return DataLoader(
        self.train_dataset,
        batch_size=self.batch_size,
        shuffle=True,
        num_workers=4
        )
  def val_dataloader(self):
    return DataLoader(
        self.val_dataset,
        batch_size=self.batch_size,
        num_workers=4
        )
  def test_dataloader(self):
    return DataLoader(
        self.test_dataset,
        batch_size=1,
        num_workers=4
        )




class ClicrModel(pl.LightningModule):
  def __init__(self):
    super().__init__()
    self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)
  def forward(self, input_ids, attention_mask, labels=None):
    output = self.model(
        input_ids, 
        attention_mask=attention_mask,
        labels=labels)
    return output.loss, output.logits
  def training_step(self, batch, batch_idx):
    input_ids = batch['input_ids']
    attention_mask=batch['attention_mask']
    labels = batch['labels']
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log("train_loss", loss, prog_bar=True, logger=True)
    return {"loss": loss, "predictions":outputs, "labels": labels}
  def validation_step(self, batch, batch_idx):
    input_ids = batch['input_ids']
    attention_mask=batch['attention_mask']
    labels = batch['labels']
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log("val_loss", loss, prog_bar=True, logger=True)
    return loss
  def test_step(self, batch, batch_idx):
    input_ids = batch['input_ids']
    attention_mask=batch['attention_mask']
    labels = batch['labels']
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log("test_loss", loss, prog_bar=True, logger=True)
    return loss
  def configure_optimizers(self):
    optimizer = AdamW(self.parameters(), lr=0.0001)
    return optimizer



from pytorch_lightning.callbacks import ModelCheckpoint

checkpoint_callback = ModelCheckpoint(
    dirpath="checkpoints",
    filename="best-checkpoint",
    save_top_k=1,
    verbose=True,
    monitor="val_loss",
    mode="min"
)
#logger = TensorBoardLogger("training-logs", name="bio-qa")
#logger = TensorBoardLogger("training-logs", name="bio-qa")

# check for the GPU provided in the runtime



# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir ./lightning_logs
if __name__ == '__main__':
    pl.seed_everything(42)
    torch.cuda.empty_cache()
    train_df = exctract_qas_CliCr(Path("C:\\Users\\SurajSREEDHARAN\\Work\\NLP\\qcb_QA\\Clicr\\clicr\\train1.0.json"))
    test_df = exctract_qas_CliCr(Path("C:\\Users\\SurajSREEDHARAN\\Work\\NLP\\qcb_QA\\Clicr\\clicr\\test1.0.json"))
    dev_df = exctract_qas_CliCr(Path("C:\\Users\\SurajSREEDHARAN\\Work\\NLP\\qcb_QA\\Clicr\\clicr\\dev1.0.json"))
    BATCH_SIZE = 2
    N_EPOCHS = 6
    data_module = ClicrDataModule(train_df, test_df, dev_df, tokenizer, batch_size=BATCH_SIZE)
    data_module.setup()
    model = ClicrModel()
    trainer = pl.Trainer(
        # logger = logger,
        # callbacks=[checkpoint_callback],
        checkpoint_callback = False,
        max_epochs=N_EPOCHS,
         gpus=1,
        progress_bar_refresh_rate=30
    )

    trainer.fit(model, data_module)
